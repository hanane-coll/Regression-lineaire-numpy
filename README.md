# Regression-lineaire-numpy
Nous allons dÃ©velopper un algorithme de descente de gradient pour rÃ©soudre un problÃ¨me de rÃ©gression linÃ©aire avec Python et sa librairie Numpy.
## 1. Importer les packages Numpy et Matplotlib.pyplot
Avant toute chose, il est nÃ©cessaire dâ€™importer les packages *Numpy* et *Matplotlib.pyplot*. *Numpy* permet de crÃ©er des matrices et effectuer des opÃ©rations mathÃ©matiques. *Matplotlib* permet de crÃ©er des graphiques pour observer facilement notre dataset ainsi que le modÃ¨le construit Ã  partir de celui-ci.
<p align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/NumPy_logo_2020.svg/1024px-NumPy_logo_2020.svg.png" width="400px"/>
</p>

<p align="center">
<img src="https://matplotlib.org/stable/_images/sphx_glr_logos2_003_2_0x.png" width="400px"/>
</p>

## 2. GÃ©nÃ©ration dâ€™un dataset (x, y) linÃ©aire
Avec la fonction linspace de Numpy, nous crÃ©ons un tableau de donnÃ©es (x, y) qui prÃ©sente une tendance linÃ©aire. La fonction *random.randn* permet dâ€™ajouter un â€œbruitâ€ alÃ©atoire normal aux donnÃ©es. Pour effectuer un calcul matriciel correct, il est important de confier 2 dimensions (100 lignes, 1 colonne) Ã  ces tableaux en utilisant la fonction *reshape(100, 1)*.
Une fois le dataset gÃ©nÃ©rÃ©, il faut ajouter une colonne de biais au tableau X, câ€™est-Ã -dire un colonne de 1, pour le dÃ©veloppement du futur modele linÃ©aire *f(x) = a*x + b*, puis initialiser des parametres a,b dans un vecteur theta.

![texte alternatif de l'image](https://www.sharetechnote.com/image/EngMath_LeastSquare_Matrix_01.png)    
*La formule matricielle du modÃ¨le de la regression linÃ©aire simple* 

## 3. DÃ©veloppement des fonctions de Descente et de gradient
Pour dÃ©velopper un modÃ¨le linÃ©aire avec la dÃ©scente de gradient, il faut implÃ©menter les 4 fonctions clefs suivantes :  
- la fonction de notre modÃ¨le : f(x) = X * ğ›‰
- la fonction Cout : J(ğ›‰) = 1\2m + (X * ğ›‰ - Y)^2
- le gradient : ğœ•J(ğ›‰)/ğœ•ğ›‰ = 1/m * X^T * (X * ğœƒ - Y)
- la descente de gradient ğ›‰ := ğ›‰ - ğ›¼ * ğœ•J(ğ›‰)/ğœ•ğ›‰

## 4. Entrainement du modÃ¨le
Une fois les fonctions ci-dessus implÃ©mentÃ©es, il suffit dâ€™utiliser la fonction gradient_descent en indiquant un nombre dâ€™itÃ©rations ainsi quâ€™un learning rate, et la fonction retournera les paramÃ¨tres du modÃ¨le aprÃ¨s entrainement, sous forme de la variable theta_final.
Vous pouvez ensuite visualiser votre modÃ¨le grÃ¢ce Ã  Matplotlib.
Pour finir, vous pouvez visualiser lâ€™Ã©volution de la descente de gradient en crÃ©ant un graphique qui trace la fonction_cout en fonction du nombre dâ€™itÃ©ration. Si votre descente de gradient a bien fonctionnÃ©, vous devez obtenir une courbe qui diminue progressivement jusquâ€™Ã  converger vers un certain minimum. Si vous nâ€™observez pas de stabilisation, alors cela signifie que le modÃ¨le nâ€™a pas terminÃ© son apprentissage et quâ€™il faut soit augmenter le nombre dâ€™itÃ©rations de la descente de gradient ou bien le pas (learning_rate).
<p align="center">
<img src="https://machinelearnia.com/wp-content/uploads/2019/07/modele-final-model-initial-1024x454.png" width="600px"/>
</p>
